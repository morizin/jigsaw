# Jigsaw - Agile Community Rules Classification

A machine learning solution for predicting whether Reddit comments violate specific subreddit rules using transformer-based models and ensemble methods.

## ğŸ† Competition Results

- **Rank**: 82nd out of 2,445 teams (Top 3.4%)
- **Medal**: Silver ğŸ¥ˆ
- **Metric**: Column-averaged AUC
- **Competition Host**: Jigsaw/Conversation AI

## ğŸ“‹ Problem Statement

This competition addresses the challenge of understanding subreddit-specific moderation. Each subreddit has unique guidelines, and determining whether a comment violates a specific rule requires understanding both the content and community context.

**Task**: Build a binary classifier that predicts whether a Reddit comment broke a specific rule from a given subreddit.

## ğŸ¯ Solution Overview

Our solution employs an ensemble approach combining:
- **Multiple transformer models** (DeBERTa, BGE embeddings)
- **Triplet learning** for improved representation learning
- **Data augmentation** techniques
- **Multi-model ensemble** (4x14B + 2x7B + 2xTriplet models)

## ğŸ“ Project Structure

```
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ config.yaml
â”œâ”€â”€ src/jigsaw/                  # Main source code
â”‚   â”œâ”€â”€ components/              # Core components
â”‚   â”‚   â”œâ”€â”€ data/               # Data processing
â”‚   â”‚   â”‚   â”œâ”€â”€ augmentation/   # Text augmentation
â”‚   â”‚   â”‚   â”œâ”€â”€ transformation/ # Data transformation
â”‚   â”‚   â”‚   â””â”€â”€ validation/     # Data validation
â”‚   â”‚   â”œâ”€â”€ dataset/            # Dataset classes
â”‚   â”‚   â”œâ”€â”€ engine/             # Training engine
â”‚   â”‚   â”œâ”€â”€ models/             # Model definitions
â”‚   â”‚   â””â”€â”€ train.py            # Training script
â”‚   â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ constants/              # Constants and prompts
â”‚   â”œâ”€â”€ core/                   # Core entities
â”‚   â”œâ”€â”€ pipelines/              # End-to-end pipelines
â”‚   â”‚   â”œâ”€â”€ data.py            # Data pipeline
â”‚   â”‚   â”œâ”€â”€ train.py           # Training pipeline
â”‚   â”‚   â””â”€â”€ inference.py       # Inference pipeline
â”‚   â””â”€â”€ utils/                  # Utility functions
â”œâ”€â”€ schemas/                    # YAML schemas
â”œâ”€â”€ scripts/                    # Utility scripts
â”œâ”€â”€ working/                    # Notebooks & experiments
â”œâ”€â”€ dist/                       # Distribution files
â”œâ”€â”€ main.py                     # Main entry point
â””â”€â”€ docker-compose.yaml         # Docker configuration
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or 3.13
- CUDA-compatible GPU (recommended)
- 16GB+ RAM

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd jigsaw-competition
```

2. **Install dependencies**
```bash
pip install uv 
uv sync
```

3. **Set up configuration**
```bash
cp config/config.yaml config/config_local.yaml
# Edit config_local.yaml with your settings
```

### Running the Pipeline
```bash
python main.py --pipeline data
```

## ğŸ”§ Configuration

Configuration files are located in the `config/` directory. Key parameters:
eight_decay: 0.01


## ğŸ“Š Model Architecture

### Base Models
- **Qwen2.5-14B**: model for better performance
- **Qwen2.5-7B**: For ensembling
- **Qwen3-14B**: Diversification
- **Qwen3-8B**: Diversification
- **DeBERTa-v3-small**: Fast and efficient transformer
- **BGE-base-en-v1.5**: Embedding model for semantic understanding
- **BGE-large-en-v1.5**: Larger embedding model for better performance

### Training Strategy
1. **Triplet Learning**: Learn better embeddings by comparing similar/dissimilar comments
2. **Cross-Validation**: 5-fold stratified cross-validation
3. **Data Augmentation**: Text augmentation for better generalization
4. **Ensemble**: Combine multiple models for robust predictions

## ğŸ”¬ Key Features

### Data Processing
- **Text Cleaning**: Remove noise, standardize formatting
- **Validation**: Ensure data quality
- **Augmentation**: Back-translation, synonym replacement
- **Zero-shot Learning**: Leverage pre-trained knowledge

### Model Training
- **Multi-GPU Support**: Distributed training capability
- **Mixed Precision**: FP16 training for faster computation
- **Learning Rate Scheduling**: Cosine annealing with warmup
- **Early Stopping**: Prevent overfitting

### Inference
- **Batch Processing**: Efficient prediction on large datasets
- **Model Ensemble**: Weighted averaging of predictions
- **TTA (Test Time Augmentation)**: Multiple predictions per sample

## ğŸ“ Scripts

Useful scripts in `scripts/`:
- `clean.sh`: Clean up temporary files and caches
- `notebook.sh`: Launch Jupyter notebook server
- `publish_kaggle.sh`: Package and publish to Kaggle
- `push.sh`: Push code to repository

## ğŸ§ª Notebooks

Experimental notebooks in `working/`:
- `training.ipynb`: Model training experiments
- `training_engine.ipynb`: Engine development
- `data_transformation.ipynb`: Data processing exploration
- `kaggle_runtime.ipynb`: Kaggle submission notebook
- `submission_runtime_v01.ipynb`: Final submission workflow

## ğŸ“š References

- [Competition Page](https://kaggle.com/competitions/jigsaw-agile-community-rules)

## ğŸ“„ License

See `LICENSE` file for details.

## ğŸ™ Acknowledgments

- **Jigsaw/Conversation AI** for hosting the competition
- **Kaggle** community for discussions and insights
- Research by Deepak Kumar, Yousef AbuHashem, Zakir Durumeric
- Dataset work by Eshwar Chandrasekharan and Eric Gilbert

## ğŸ“§ Contact

For questions or collaboration:
- Kaggle: morizin
- GitHub: morizin

---

**Note**: This project was developed for the Kaggle competition "Jigsaw - Agile Community Rules Classification" and achieved a Silver Medal (82nd place out of 2,445 teams).