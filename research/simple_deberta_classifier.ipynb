{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de297155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"/Users/morizin/Documents/Code/jigsaw-competition\")\n",
    "\n",
    "import re\n",
    "from src.jigsaw.utils.common import read_csv\n",
    "from src.jigsaw import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a70e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/morizin/Documents/Code/jigsaw-competition\")\n",
    "\n",
    "from src.jigsaw.config.config import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583037b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 18:15:21,051 [INFO] : common : Loading YAML file : ./config/config.yaml\n",
      "2025-09-30 18:15:21,137 [INFO] : common : Loading YAML file : ./params.yaml\n",
      "2025-09-30 18:15:21,140 [INFO] : common : Loading YAML file : ./schema.yaml\n",
      "2025-09-30 18:15:21,143 [INFO] : common : Loading Json file : artifacts/data/status.json\n",
      "2025-09-30 18:15:21,145 [INFO] : common : Successfully loaded file: artifacts/data/status.json -> 1 master keys.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for EngineParams\nbatch_size\n  Field required [type=missing, input_value={'model_name': 'microsoft...rue, padding='longest')}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m cfg = ConfigurationManager()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model_training_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/jigsaw-competition/src/jigsaw/config/config.py:212\u001b[39m, in \u001b[36mConfigurationManager.get_model_training_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    209\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrainingArguments \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.engine\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m engine_params = \u001b[43mEngineParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgradient_accumulation_steps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwarmup_ratio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTokenizerParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m schemas = []\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.names:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/jigsaw-competition/.venv/lib/python3.13/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for EngineParams\nbatch_size\n  Field required [type=missing, input_value={'model_name': 'microsoft...rue, padding='longest')}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "cfg = ConfigurationManager()\n",
    "cfg.get_model_training_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    fold : int = 4\n",
    "    dataset : list[str] = ['artifacts/data/folded_cleaned_raw']\n",
    "    files: list[str] = ['train.csv']\n",
    "    url2sem : bool = False\n",
    "    features : list[str] = ['body', 'subreddit', 'rule', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']\n",
    "    labels : list[str] | str = 'rule_violation'\n",
    "\n",
    "\n",
    "    truncation : bool | str = True\n",
    "    model_name : str = 'microsoft/deberta-v3-small'\n",
    "    padding : bool | str = 'max_length'\n",
    "    max_length : int = 2048\n",
    "\n",
    "    outdir : str = './model'\n",
    "    nepochs : int = 1\n",
    "    learning_rate : float = 2e-5\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_step : int = 1\n",
    "    weight_decay : float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "\n",
    "config = CFG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "import os\n",
    "\n",
    "def get_datas(config: CFG) -> DataFrame:\n",
    "    data_coll = []\n",
    "    for dataset in config.dataset:\n",
    "        for file in config.files:\n",
    "            data = read_csv(os.path.join(dataset, file))\n",
    "            columns = config.features\n",
    "            if all([col in data.columns for col in columns]):\n",
    "                data_coll.append(data)\n",
    "            else:\n",
    "                logger.error(\n",
    "                    f\"The dataset can't be inlcuded as it have unmatched columns names {data.columns}\"\n",
    "                )\n",
    "    data = pd.concat(data_coll, axis=0)\n",
    "    return data\n",
    "\n",
    "data = get_datas(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from ensure import ensure_annotations\n",
    "from pandas.core.frame import DataFrame\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2 import DebertaV2Tokenizer\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import (\n",
    "    DebertaV2TokenizerFast,\n",
    ")\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.data import build_prompt, url_to_semantics\n",
    "from src.jigsaw.utils.common import read_csv\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClassifierDataset(Dataset):\n",
    "    @ensure_annotations\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: CFG,\n",
    "        data: DataFrame | str,\n",
    "        tokenizer: DebertaV2Tokenizer | DebertaV2TokenizerFast | str,\n",
    "    ):\n",
    "        if isinstance(data, str):\n",
    "            data = read_csv(data)\n",
    "\n",
    "        if isinstance(data, DataFrame):\n",
    "            self.data = data\n",
    "        else:\n",
    "            error = f\"'data' can be either str or pd.DataFrame. 'data' has type {type(data).__name__}\"\n",
    "            logger.error(error)\n",
    "            raise Exception(error)\n",
    "\n",
    "        if isinstance(tokenizer, str):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "            \n",
    "        if isinstance(tokenizer, (DebertaV2Tokenizer, DebertaV2TokenizerFast)):\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            error = f\"'tokenizer' can be either str, DebertaV2Tokenizer, DebertaV2TokenizerFast. 'tokenizer' has type {type(tokenizer).__name__}\"\n",
    "            logger.error(error)\n",
    "            raise Exception(error)\n",
    "\n",
    "        if config.url2sem:\n",
    "            for (col, dtype) in data.dtypes.items():\n",
    "                if is_string_dtype(dtype):\n",
    "                    data[col] = data[col] + data[col].apply(url_to_semantics)\n",
    "\n",
    "        self.completion = data.apply(build_prompt, axis=1).to_list()\n",
    "        \n",
    "        self.encoding = self.tokenizer(\n",
    "            self.completion, truncation=config.truncation, padding = config.padding, max_length=config.max_length\n",
    "        )\n",
    "\n",
    "        if isinstance(config.labels, str):\n",
    "            config.labels = [config.labels]\n",
    "\n",
    "        if any([col in data.columns for col in config.labels]):\n",
    "            self.labels = data[config.labels].to_numpy()\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self,) -> int:\n",
    "        assert len(self.encoding['input_ids']) == len(self.labels), f\"Input and Output length mismatch {len(self.encoding)} != {len(self.labels)}\"\n",
    "        return len(self.encoding['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]: \n",
    "        items = {key : torch.tensor(value[idx]) for (key, value) in self.encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            items['label'] = torch.tensor(self.labels[idx].flatten())\n",
    "        return items\n",
    "\n",
    "\n",
    "train_dataset = ClassifierDataset(\n",
    "    config, data, config.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=16,\n",
    "#     shuffle = True,\n",
    "#     num_workers=0,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# for i in train_dataloader:\n",
    "#     print(i['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a521735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.model_name, trust_remote_code = True)\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "del model.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968280f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=config.outdir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps= config.gradient_accumulation_step,\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    warmup_ratio= config.warmup_ratio,\n",
    "    num_train_epochs= config.nepochs,\n",
    "    report_to= 'none',\n",
    "    save_strategy='no'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0486a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jigsaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
