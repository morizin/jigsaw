{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7af18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b39a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.config_entity import DataTransformationConfig, DataSplitParams\n",
    "\n",
    "from src.jigsaw.components.data.cleaning import remove_duplicates, clean_text\n",
    "from src.jigsaw.components.data.zeroshot import zero_shot_transform\n",
    "from src.jigsaw.components.data.folding import split_dataset\n",
    "from pathlib import Path\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_csv, print_format\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "        self.pipeline = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                self.pipeline.append((final_dir, remove_duplicates))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                self.pipeline.append((final_dir, clean_text))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                self.pipeline.append((final_dir, list))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_\" + final_dir\n",
    "                self.pipeline.append((final_dir, zero_shot_transform))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.pairwise:\n",
    "            for name in self.names:\n",
    "                final_dir = \"pairwise_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.splitter:\n",
    "            for name in self.names:\n",
    "                final_dir = \"folded_\" + final_dir\n",
    "                self.pipeline.append((final_dir, split_dataset))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        for name in self.names:\n",
    "            for path in (self.indir / name).iterdir():\n",
    "                data = read_csv(path)\n",
    "                path = str(path).split(\"/\")[-2:]\n",
    "                for (dirname, process) in self.pipeline:\n",
    "                    data = process(\n",
    "                        config= self.config,\n",
    "                        data = data,\n",
    "                        path = path,\n",
    "                        name = dirname + name,\n",
    "                        outdir = self.outdir\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259be495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "\n",
    "cfg = ConfigurationManager()\n",
    "DataTransformationComponent(cfg.get_data_transformation_config())()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7446ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "# from src.jigsaw.components.data_transform import DataTranformationComponent\n",
    "\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.config = ConfigurationManager().get_data_transformation_config()\n",
    "        self.comp = DataTransformationComponent(self.config)\n",
    "\n",
    "    def kickoff(\n",
    "        self,\n",
    "    ):\n",
    "        self.comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTransformationPipeline().kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.config_entity import DataTransformationConfig, DataSplitParams\n",
    "\n",
    "from src.jigsaw.components.dataset.cleaning import remove_duplicates\n",
    "from pathlib import Path\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_csv, print_format\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_shot_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.pairwise:\n",
    "            for name in self.names:\n",
    "                final_dir = \"pairwise\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        for dataset in self.names:\n",
    "            for file in (self.indir / dataset).iterdir():\n",
    "                final_dir = dataset\n",
    "                data = read_csv(file)\n",
    "                file = str(file).split(\"/\")[-2:]\n",
    "                if self.config.features:\n",
    "                    final_dir = \"cleaned_\" + final_dir\n",
    "                    data = remove_duplicates(\n",
    "                        self.config, data, file, final_dir, self.outdir\n",
    "                    )\n",
    "\n",
    "                if self.config.wash:\n",
    "                    final_dir = \"washed_\" + final_dir\n",
    "                    data = self.clean_text(data, file, name=final_dir)\n",
    "\n",
    "                if self.config.zero:\n",
    "                    final_dir = \"zero_\" + final_dir\n",
    "                    data = self.zero_shot_transform(data, file, name=final_dir)\n",
    "\n",
    "    @ensure_annotations\n",
    "    def deduplication(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        print(path, filename)\n",
    "        if filename[-1] != \"sample_submission.csv\":\n",
    "            features = self.config.features[filename[0]]\n",
    "            try:\n",
    "                data.drop_duplicates(subset=features, ignore_index=True, inplace=True)\n",
    "                logger.info(f\"cleaning out duplicates: {'.'.join(filename)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Failed cleaning out duplicates: {'.'.join(filename)}\\nManual Cleaning\"\n",
    "                )\n",
    "                data.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "\n",
    "    @ensure_annotations\n",
    "    def clean_text(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        def clean_text(text):\n",
    "            return clean(\n",
    "                text,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=False,\n",
    "                no_line_breaks=False,\n",
    "                no_urls=True,\n",
    "                no_emails=True,\n",
    "                no_phone_numbers=True,\n",
    "                no_numbers=False,\n",
    "                no_digits=False,\n",
    "                no_currency_symbols=False,\n",
    "                no_punct=False,\n",
    "                no_emoji=True,\n",
    "                replace_with_url=\"<URL>\",\n",
    "                replace_with_phone_number=\"<PHONE>\",\n",
    "                replace_with_email=\"<EMAIL>\",\n",
    "            )\n",
    "\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "\n",
    "        if \"sample_submission.csv\" not in filename:\n",
    "            data[\"body\"] = data[\"body\"].apply(clean_text)\n",
    "            for key, dtype in data.dtypes.items():\n",
    "                if is_string_dtype(dtype):\n",
    "                    data[key] = data[key].apply(clean_text)\n",
    "            logger.info(f\"Washed the file : {'.'.join(filename)}\")\n",
    "            self.deduplication(data, path, name=\"\", save=False)\n",
    "        else:\n",
    "            logger.warning(f\"Couldn't clean text in {'.'.join(filename)}\")\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "\n",
    "    @ensure_annotations\n",
    "    def zero_shot_transform(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        try:\n",
    "            if \"sample_submission.csv\" not in filename:\n",
    "                features = self.config.features[filename[0]]\n",
    "\n",
    "                try:\n",
    "                    zeroshot = [data[features + [\"rule_violation\"]]]\n",
    "                except KeyError:\n",
    "                    zeroshot = []\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "                for violation in [\"positive\", \"negative\"]:\n",
    "                    for i in range(1, 3):\n",
    "                        temp = data[features[:-1] + [f\"{violation}_example_{i}\"]]\n",
    "                        temp[\"rule_violation\"] = 1 if violation == \"positive\" else 0\n",
    "                        temp = temp.rename(columns={f\"{violation}_example_{i}\": \"body\"})\n",
    "                        zeroshot.append(temp)\n",
    "\n",
    "                zeroshot = pd.concat(zeroshot, axis=0)\n",
    "                logger.info(f\"Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\")\n",
    "            else:\n",
    "                zeroshot = data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Error Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(zeroshot, target_dir.path / filename[-1])\n",
    "        return zeroshot\n",
    "\n",
    "    # @ensure_annotations\n",
    "    # def split_dataset(self, data: pd.DataFrame, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jigsaw-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
