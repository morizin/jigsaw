{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7af18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b39a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/morizin/Documents/Code/jigsaw-competition'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53915642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.common import FilePath\n",
    "\n",
    "\n",
    "class DataSplitParams(BaseModel):\n",
    "    type: str\n",
    "    nsplits: int = 5\n",
    "    random_state: int = 2025\n",
    "\n",
    "\n",
    "class DataTransformationConfig(BaseModel):\n",
    "    outdir: Directory\n",
    "    indir: Directory\n",
    "    datasets: list[str]\n",
    "    splitter: DataSplitParams\n",
    "    features: dict[str, list[str]] | None\n",
    "    wash: bool\n",
    "    triplet: bool\n",
    "    zero: bool\n",
    "    matching: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morizin/Documents/Code/jigsaw-competition/.venv/lib/python3.13/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"schema\" in \"DataSchema\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from box import ConfigBox\n",
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "from src.jigsaw.utils.common import load_yaml, load_json\n",
    "from src.jigsaw.constants import *\n",
    "from src.jigsaw.entity.common import FilePath, Directory\n",
    "import os\n",
    "from src.jigsaw import logger\n",
    "# from src.jigsaw.entity.config_entity import DataSplitParams, DataTransformationConfig\n",
    "\n",
    "\n",
    "class ConfigurationManager(ConfigurationManager):\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        split_config = self.params.splitter\n",
    "        data_transform = self.config.data_transformation\n",
    "        splitter = DataSplitParams(\n",
    "            type=split_config.type,\n",
    "            nsplits=split_config.nsplits,\n",
    "            random_state=split_config.random_state,\n",
    "        )\n",
    "        status_file = load_json(\n",
    "            self.artifact_root.path\n",
    "            / os.path.join(self.config.data_validation.outdir, \"status.json\")\n",
    "        )\n",
    "\n",
    "        features = dict()\n",
    "        names = []\n",
    "        for name, schema in self.schema.items():\n",
    "            names.append(name)\n",
    "            if name in status_file:\n",
    "                for value in status_file[name].values():\n",
    "                    print(value)\n",
    "                    if value[\"data_redundancy\"]:\n",
    "                        features[name] = schema.features\n",
    "\n",
    "        return DataTransformationConfig(\n",
    "            outdir=Directory(path=self.artifact_root.path / data_transform.outdir),\n",
    "            indir=Directory(\n",
    "                path=self.artifact_root.path / self.config.data_ingestion.outdir\n",
    "            ),\n",
    "            datasets= names,\n",
    "            splitter=splitter,\n",
    "            features=features,\n",
    "            wash=data_transform.wash if hasattr(data_transform, \"wash\") else False,\n",
    "            triplet=data_transform.triplet\n",
    "            if hasattr(data_transform, \"triplet\")\n",
    "            else False,\n",
    "            zero=data_transform.zero if hasattr(data_transform, \"zero\") else False,\n",
    "            matching=data_transform.matching\n",
    "            if hasattr(data_transform, \"matching\")\n",
    "            else False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9e3e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.common import FilePath\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_json, save_csv, print_format\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print('='*length)\n",
    "        \n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print('='*length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print('='*length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_shot_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print('='*length)\n",
    "\n",
    "        if self.config.matching:\n",
    "            for name in self.names:\n",
    "                final_dir = \"matching_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print('='*length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        \n",
    "        for dataset in self.names:\n",
    "            for file in (self.indir / dataset).iterdir():\n",
    "                final_dir = dataset\n",
    "                data = read_csv(file)\n",
    "                if self.config.features:\n",
    "                    final_dir = \"cleaned_\" + final_dir \n",
    "                    data = self.deduplication(data, file, name = final_dir)\n",
    "                \n",
    "                if self.config.wash:\n",
    "                    final_dir = \"washed_\" + final_dir\n",
    "                    data = self.clean_text(data, file, name = final_dir)\n",
    "\n",
    "                if self.config.zero:\n",
    "                    final_dir = \"zero_\" + final_dir\n",
    "                    data = self.zero_shot_transform(data, file, name = final_dir)\n",
    "\n",
    "    @ensure_annotations\n",
    "    def deduplication(self, data: pd.DataFrame, path: Path | str,  name :str, save : bool = True) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        \n",
    "        if filename[-1] != 'sample_submission.csv':\n",
    "            features = self.config.features[filename[0]]\n",
    "            try:\n",
    "                data.drop_duplicates(\n",
    "                    subset=features,\n",
    "                    ignore_index=True, \n",
    "                    inplace=True\n",
    "                )\n",
    "                logger.info(f\"cleaning out duplicates: {'.'.join(filename)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed cleaning out duplicates: {'.'.join(filename)}\\nManual Cleaning\")\n",
    "                data.drop_duplicates(ignore_index=True, inplace=True)\n",
    "        \n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir/ name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "    \n",
    "\n",
    "    @ensure_annotations\n",
    "    def clean_text(self, data: pd.DataFrame, path : Path | str, name : str, save : bool = True) -> pd.DataFrame:\n",
    "        def clean_text(text):\n",
    "            return clean(\n",
    "                text,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=False,\n",
    "                no_line_breaks=False,\n",
    "                no_urls=True,\n",
    "                no_emails=True,\n",
    "                no_phone_numbers=True,\n",
    "                no_numbers=False,\n",
    "                no_digits=False,\n",
    "                no_currency_symbols=False,\n",
    "                no_punct=False,\n",
    "                no_emoji=True,\n",
    "                replace_with_url=\"<URL>\",\n",
    "                replace_with_phone_number=\"<PHONE>\",\n",
    "                replace_with_email=\"<EMAIL>\"\n",
    "            )\n",
    "        \n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "    \n",
    "        if 'sample_submission.csv' not in filename:\n",
    "            data['body'] = data['body'].apply(clean_text)\n",
    "            for (key, dtype) in data.dtypes.items():\n",
    "                if is_string_dtype(dtype):\n",
    "                    data[key] = data[key].apply(clean_text)\n",
    "            logger.info(f\"Washed the file : {'.'.join(filename)}\")\n",
    "            self.deduplication(data, path, name = \"\", save = False)\n",
    "        else:\n",
    "            logger.warning(f\"Couldn't clean text in {'.'.join(filename)}\")\n",
    "        \n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir/ name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "    \n",
    "    @ensure_annotations\n",
    "    def zero_shot_transform(self, data: pd.DataFrame, path : Path | str, name : str, save : bool = True) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        try:\n",
    "            if 'sample_submission.csv' not in filename:\n",
    "                features = self.config.features[filename[0]]\n",
    "                try:\n",
    "                    zeroshot = [data[features + ['rule_violation']]]\n",
    "                except KeyError:\n",
    "                    zeroshot = []\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "                \n",
    "                for violation in ['positive', 'negative']:\n",
    "                    for i in range(1, 3):\n",
    "                        temp = data[features[:-1] + [f\"{violation}_example_{i}\"]]\n",
    "                        temp ['rule_violation'] = 1 if violation == \"positive\" else 0\n",
    "                        temp = temp.rename(columns= { f\"{violation}_example_{i}\": 'body'})\n",
    "                        zeroshot.append(temp)\n",
    "\n",
    "                zeroshot = pd.concat(zeroshot, axis=0)\n",
    "                logger.info(f\"Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\")\n",
    "            else:\n",
    "                zeroshot = data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\")\n",
    "            raise e\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path = self.outdir / name)\n",
    "            save_csv(zeroshot, target_dir.path / filename[-1])\n",
    "        return zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c86caf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-26 00:43:14,283 [INFO] : common : Loading YAML file : ./config/config.yaml\n",
      "2025-09-26 00:43:14,288 [INFO] : common : Loading YAML file : ./params.yaml\n",
      "2025-09-26 00:43:14,291 [INFO] : common : Loading YAML file : ./schema.yaml\n",
      "2025-09-26 00:43:14,295 [INFO] : common : Loading Json file : artifacts/data/status.json\n",
      "2025-09-26 00:43:14,295 [INFO] : common : Successfully loaded file: artifacts/data/status.json -> 1 master keys.\n",
      "{'missing_values': False, 'mismatch_dtype': False, 'data_redundancy': True}\n",
      "{'missing_values': False, 'mismatch_dtype': False, 'data_redundancy': False}\n",
      "====================================================================================================\n",
      "|                                        Datasets Available                                        |\n",
      "====================================================================================================\n",
      "|                                                raw                                                |\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "|                                        Datasets Generating                                        |\n",
      "====================================================================================================\n",
      "|                                    artifacts/data/cleaned_raw                                    |\n",
      "====================================================================================================\n",
      "|                                 artifacts/data/washed_cleaned_raw                                 |\n",
      "====================================================================================================\n",
      "|                            artifacts/data/zero_shot_washed_cleaned_raw                            |\n",
      "====================================================================================================\n",
      "2025-09-26 00:43:14,297 [INFO] : common : Reading artifacts/data/raw/test.csv file ...\n",
      "2025-09-26 00:43:14,306 [INFO] : common : Successfully read the CSV artifacts/data/raw/test.csv : (10, 8)\n",
      "2025-09-26 00:43:14,309 [INFO] : 371715732 : cleaning out duplicates: raw.test.csv\n",
      "2025-09-26 00:43:14,309 [INFO] : common : Saving dataframe of (10, 8) into artifacts/data/cleaned_raw/test.csv. ...\n",
      "2025-09-26 00:43:14,318 [INFO] : common : Saved the data into artifacts/data/cleaned_raw/test.csv\n",
      "2025-09-26 00:43:14,329 [INFO] : 371715732 : Washed the file : raw.test.csv\n",
      "2025-09-26 00:43:14,330 [INFO] : 371715732 : cleaning out duplicates: raw.test.csv\n",
      "2025-09-26 00:43:14,330 [INFO] : common : Saving dataframe of (10, 8) into artifacts/data/washed_cleaned_raw/test.csv. ...\n",
      "2025-09-26 00:43:14,331 [INFO] : common : Saved the data into artifacts/data/washed_cleaned_raw/test.csv\n",
      "2025-09-26 00:43:14,334 [INFO] : 371715732 : Tranforming to Zero-Shot Dataset : raw.test.csv\n",
      "2025-09-26 00:43:14,335 [INFO] : common : Saving dataframe of (40, 4) into artifacts/data/zero_washed_cleaned_raw/test.csv. ...\n",
      "2025-09-26 00:43:14,336 [INFO] : common : Saved the data into artifacts/data/zero_washed_cleaned_raw/test.csv\n",
      "2025-09-26 00:43:14,336 [INFO] : common : Reading artifacts/data/raw/train.csv file ...\n",
      "2025-09-26 00:43:14,363 [INFO] : common : Successfully read the CSV artifacts/data/raw/train.csv : (2029, 9)\n",
      "2025-09-26 00:43:14,366 [INFO] : 371715732 : cleaning out duplicates: raw.train.csv\n",
      "2025-09-26 00:43:14,366 [INFO] : common : Saving dataframe of (1938, 9) into artifacts/data/cleaned_raw/train.csv. ...\n",
      "2025-09-26 00:43:14,400 [INFO] : common : Saved the data into artifacts/data/cleaned_raw/train.csv\n",
      "2025-09-26 00:43:15,994 [INFO] : 371715732 : Washed the file : raw.train.csv\n",
      "2025-09-26 00:43:15,997 [INFO] : 371715732 : cleaning out duplicates: raw.train.csv\n",
      "2025-09-26 00:43:15,998 [INFO] : common : Saving dataframe of (1913, 9) into artifacts/data/washed_cleaned_raw/train.csv. ...\n",
      "2025-09-26 00:43:16,027 [INFO] : common : Saved the data into artifacts/data/washed_cleaned_raw/train.csv\n",
      "2025-09-26 00:43:16,031 [INFO] : 371715732 : Tranforming to Zero-Shot Dataset : raw.train.csv\n",
      "2025-09-26 00:43:16,032 [INFO] : common : Saving dataframe of (9565, 4) into artifacts/data/zero_washed_cleaned_raw/train.csv. ...\n",
      "2025-09-26 00:43:16,075 [INFO] : common : Saved the data into artifacts/data/zero_washed_cleaned_raw/train.csv\n",
      "2025-09-26 00:43:16,075 [INFO] : common : Reading artifacts/data/raw/sample_submission.csv file ...\n",
      "2025-09-26 00:43:16,079 [INFO] : common : Successfully read the CSV artifacts/data/raw/sample_submission.csv : (10, 2)\n",
      "2025-09-26 00:43:16,080 [INFO] : common : Saving dataframe of (10, 2) into artifacts/data/cleaned_raw/sample_submission.csv. ...\n",
      "2025-09-26 00:43:16,081 [INFO] : common : Saved the data into artifacts/data/cleaned_raw/sample_submission.csv\n",
      "2025-09-26 00:43:16,081 [WARNING] : 371715732 : Couldn't clean text in raw.sample_submission.csv\n",
      "2025-09-26 00:43:16,082 [INFO] : common : Saving dataframe of (10, 2) into artifacts/data/washed_cleaned_raw/sample_submission.csv. ...\n",
      "2025-09-26 00:43:16,083 [INFO] : common : Saved the data into artifacts/data/washed_cleaned_raw/sample_submission.csv\n",
      "2025-09-26 00:43:16,083 [INFO] : common : Saving dataframe of (10, 2) into artifacts/data/zero_washed_cleaned_raw/sample_submission.csv. ...\n",
      "2025-09-26 00:43:16,084 [INFO] : common : Saved the data into artifacts/data/zero_washed_cleaned_raw/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "DataTransformationComponent(ConfigurationManager().get_data_transformation_config())()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7446ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.jigsaw.config.config import ConfigurationManager\n",
    "# from src.jigsaw.components.data_transform import DataTranformationComponent\n",
    "\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.config = ConfigurationManager().get_data_transformation_config()\n",
    "        self.comp = DataTransformationComponent(self.config)\n",
    "\n",
    "    def kickoff(\n",
    "        self,\n",
    "    ):\n",
    "        self.comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220f8747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 23:58:56,302 [INFO] : common : Loading YAML file : ./config/config.yaml\n",
      "2025-09-25 23:58:56,306 [INFO] : common : Loading YAML file : ./params.yaml\n",
      "2025-09-25 23:58:56,308 [INFO] : common : Loading YAML file : ./schema.yaml\n",
      "2025-09-25 23:58:56,310 [INFO] : common : Loading Json file : artifacts/data/status.json\n",
      "2025-09-25 23:58:56,311 [INFO] : common : Successfully loaded file: artifacts/data/status.json -> 1 master keys.\n",
      "{'missing_values': False, 'mismatch_dtype': False, 'data_redundancy': True}\n",
      "{'missing_values': False, 'mismatch_dtype': False, 'data_redundancy': False}\n",
      "====================================================================================================\n",
      "|                                        Datasets Available                                        |\n",
      "====================================================================================================\n",
      "|                                                raw                                                |\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "|                                        Datasets Generating                                        |\n",
      "====================================================================================================\n",
      "|                                    artifacts/data/cleaned_raw                                    |\n",
      "====================================================================================================\n",
      "2025-09-25 23:58:56,312 [INFO] : common : Reading artifacts/data/raw/test.csv file ...\n",
      "2025-09-25 23:58:56,314 [INFO] : common : Successfully read the CSV artifacts/data/raw/test.csv : (10, 8)\n",
      "2025-09-25 23:58:56,315 [INFO] : 2480858423 : cleaning out duplicates: raw.test.csv\n",
      "2025-09-25 23:58:56,315 [INFO] : common : Saving dataframe of (10, 8) into artifacts/data/cleaned_raw/test.csv. ...\n",
      "2025-09-25 23:58:56,318 [INFO] : common : Saved the data into artifacts/data/cleaned_raw/test.csv\n",
      "2025-09-25 23:58:56,318 [INFO] : common : Reading artifacts/data/raw/train.csv file ...\n",
      "2025-09-25 23:58:56,347 [INFO] : common : Successfully read the CSV artifacts/data/raw/train.csv : (2029, 9)\n",
      "2025-09-25 23:58:56,351 [INFO] : 2480858423 : cleaning out duplicates: raw.train.csv\n",
      "2025-09-25 23:58:56,351 [INFO] : common : Saving dataframe of (1938, 9) into artifacts/data/cleaned_raw/train.csv. ...\n",
      "2025-09-25 23:58:56,384 [INFO] : common : Saved the data into artifacts/data/cleaned_raw/train.csv\n",
      "2025-09-25 23:58:56,385 [INFO] : common : Reading artifacts/data/raw/sample_submission.csv file ...\n",
      "2025-09-25 23:58:56,386 [INFO] : common : Successfully read the CSV artifacts/data/raw/sample_submission.csv : (10, 2)\n",
      "2025-09-25 23:58:56,387 [INFO] : common : Saving dataframe of (10, 2) into artifacts/data/cleaned_raw/sample_submission.csv. ...\n",
      "2025-09-25 23:58:56,388 [INFO] : common : Saved the data into artifacts/data/cleaned_raw/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "DataTransformationPipeline().kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87bc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jigsaw-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
