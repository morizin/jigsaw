{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7af18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b39a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.config_entity import DataTransformationConfig, DataSplitParams\n",
    "\n",
    "from src.jigsaw.components.data.cleaning import remove_duplicates, clean_text\n",
    "from src.jigsaw.components.data.zeroshot import zero_shot_transform\n",
    "from src.jigsaw.components.data.folding import split_dataset\n",
    "from pathlib import Path\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_csv, print_format\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "        self.pipeline = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                self.pipeline.append((final_dir, remove_duplicates))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                self.pipeline.append((final_dir, clean_text))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_\" + final_dir\n",
    "                self.pipeline.append((final_dir, zero_shot_transform))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                self.pipeline.append((final_dir, list))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.pairwise:\n",
    "            for name in self.names:\n",
    "                final_dir = \"pairwise_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.splitter:\n",
    "            for name in self.names:\n",
    "                final_dir = \"folded_\" + final_dir\n",
    "                self.pipeline.append((final_dir, split_dataset))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        for name in self.names:\n",
    "            for path in (self.indir / name).iterdir():\n",
    "                data = read_csv(path)\n",
    "                path = str(path).split(\"/\")[-2:]\n",
    "                for (dirname, process) in self.pipeline:\n",
    "                    data = process(\n",
    "                        config= self.config,\n",
    "                        data = data,\n",
    "                        path = path,\n",
    "                        name = dirname + name,\n",
    "                        outdir = self.outdir\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/Users/morizin/Documents/Code/jigsaw-competition\")\n",
    "\n",
    "data = pd.read_csv(\"artifacts/data/zero_washed_cleaned_raw/train.csv\")\n",
    "\n",
    "((_, neg), (_, pos)) = data.groupby(by = 'rule_violation')\n",
    "pos = pos.reset_index(drop = True)\n",
    "neg = neg.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a38f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataConfig(BaseModel):\n",
    "    ntriplets : int\n",
    "    nsamples  : int\n",
    "    random_state : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jigsaw.entity.config_entity import DataTransformationConfig\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.utils.common import save_csv\n",
    "from pandas.core.frame import DataFrame\n",
    "from ensure import ensure_annotations\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.jigsaw import logger\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "@ensure_annotations\n",
    "def triplet_dataset(config: DataTransformationComponent,\n",
    "                    data: DataFrame, \n",
    "                    path : list,\n",
    "                    name : str,\n",
    "                    outdir: str | Path | None = None) -> DataFrame:\n",
    "\n",
    "    dataname, filename = path\n",
    "    triplet_config = config.triplet\n",
    "\n",
    "    if filename == 'sample_submission.csv':\n",
    "        return data\n",
    "    \n",
    "    if set(data.columns) != set(config.features + [config.targets]):\n",
    "        return data\n",
    "    \n",
    "    if data.loc[0, 'rule_violation'] == 0:\n",
    "        ((_, neg), (_, pos)) = data.groupby('rule_violation')\n",
    "    else:\n",
    "        ((_, pos), (_, neg)) = data.groupby(\"rule_violation\")\n",
    "\n",
    "    pos = pos.reset_index(drop = True)\n",
    "    neg = neg.reset_index(drop = True)\n",
    "\n",
    "    rules = neg.groupby('rule').apply(\n",
    "        lambda x: list(\n",
    "            x.sample(\n",
    "                len(x)\n",
    "            ).index\n",
    "        )\n",
    "    ).to_dict()\n",
    "\n",
    "    sr = neg.groupby(['rule', 'subreddit']).apply(\n",
    "        lambda x: list(\n",
    "            x.sample(\n",
    "                len(x)\n",
    "            ).index\n",
    "        )\n",
    "    ).to_dict()\n",
    "\n",
    "    pos_repeat = pd.concat([pos] * triplet_config.ntriplets, axis = 0)\n",
    "    negatives = []\n",
    "\n",
    "    logger.info(f\"Generating {triplet_config.nsamples} samples of {triplet_config.ntriplet} triplets each in the file {dataname}.{filename}\")\n",
    "    for idx, positive in tqdm(pos_repeat.iterrows(), total = len(pos_repeat)):\n",
    "        subred = sr.get((positive.rule, positive.subreddit), None)\n",
    "        chosen_idx = []\n",
    "        remaining = triplet_config.nsample\n",
    "        if subred:\n",
    "            idx = min(len(subred), remaining)\n",
    "            chosen_idx.extend(subred[:idx])\n",
    "            sr[(positive.rule, positive.subreddit)] = subred[idx:]\n",
    "            remaining -= idx\n",
    "\n",
    "        if remaining:\n",
    "            rule = rules[positive.rule]\n",
    "            idx = min(remaining, len(rule))\n",
    "            chosen_idx.extend(rule[:idx]) \n",
    "            rules[positive.rule] = rule[idx:]\n",
    "            remaining -= idx\n",
    "            \n",
    "        while remaining > 0:\n",
    "            rules = neg.groupby('rule').apply(\n",
    "                lambda x: list(\n",
    "                    x.sample(\n",
    "                        len(x)\n",
    "                    ).index\n",
    "                )\n",
    "            ).to_dict()\n",
    "            rule = rules[positive.rule]\n",
    "            idx = min(remaining, len(rule))\n",
    "            chosen_idx.extend(rule[:idx]) \n",
    "            rules[positive.rule] = rule[idx:]\n",
    "            remaining -= idx\n",
    "\n",
    "        negatives.append(chosen_idx)\n",
    "\n",
    "    negatives = pd.DataFrame([neg.loc[idx, 'body'].values for idx in negatives], columns = [f\"negative_{i}\" for i in range(len(negatives[0]))], index = range(len(negatives)))\n",
    "\n",
    "    assert negatives.shape == (pos.shape[0] * triplet_config.ntriplets, triplet_config.nsample), logger.error(f\"Error when generating triplets '{dataname}.{filename}', shape doesn't match {negatives.shape} != ({pos.shape[0] * triplet_config.ntriplets}, {triplet_config.nsamples}\")\n",
    "\n",
    "    logger.info(f\"Generating {triplet_config.nsamples} samples of {triplet_config.ntriplet} triplets each in the file {dataname}.{filename}\")\n",
    "\n",
    "    pos_repeat = pd.merge(pos_repeat, negatives)\n",
    "\n",
    "    if outdir:\n",
    "        target_dir = Directory(path = (Path(outdir) if isinstance(outdir, str) else outdir) / name)\n",
    "        save_csv(data, target_dir.path / filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259be495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/morizin/Documents/Code/jigsaw-competition\")\n",
    "\n",
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "from src.jigsaw.components.data.transformation import DataTransformationComponent\n",
    "\n",
    "cfg = ConfigurationManager()\n",
    "DataTransformationComponent(cfg.get_data_transformation_config())()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7446ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "# from src.jigsaw.components.data_transform import DataTranformationComponent\n",
    "\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.config = ConfigurationManager().get_data_transformation_config()\n",
    "        self.comp = DataTransformationComponent(self.config)\n",
    "\n",
    "    def kickoff(\n",
    "        self,\n",
    "    ):\n",
    "        self.comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTransformationPipeline().kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.config_entity import DataTransformationConfig, DataSplitParams\n",
    "\n",
    "from src.jigsaw.components.dataset.cleaning import remove_duplicates\n",
    "from pathlib import Path\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_csv, print_format\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_shot_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.pairwise:\n",
    "            for name in self.names:\n",
    "                final_dir = \"pairwise\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        for dataset in self.names:\n",
    "            for file in (self.indir / dataset).iterdir():\n",
    "                final_dir = dataset\n",
    "                data = read_csv(file)\n",
    "                file = str(file).split(\"/\")[-2:]\n",
    "                if self.config.features:\n",
    "                    final_dir = \"cleaned_\" + final_dir\n",
    "                    data = remove_duplicates(\n",
    "                        self.config, data, file, final_dir, self.outdir\n",
    "                    )\n",
    "\n",
    "                if self.config.wash:\n",
    "                    final_dir = \"washed_\" + final_dir\n",
    "                    data = self.clean_text(data, file, name=final_dir)\n",
    "\n",
    "                if self.config.zero:\n",
    "                    final_dir = \"zero_\" + final_dir\n",
    "                    data = self.zero_shot_transform(data, file, name=final_dir)\n",
    "\n",
    "    @ensure_annotations\n",
    "    def deduplication(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        print(path, filename)\n",
    "        if filename[-1] != \"sample_submission.csv\":\n",
    "            features = self.config.features[filename[0]]\n",
    "            try:\n",
    "                data.drop_duplicates(subset=features, ignore_index=True, inplace=True)\n",
    "                logger.info(f\"cleaning out duplicates: {'.'.join(filename)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Failed cleaning out duplicates: {'.'.join(filename)}\\nManual Cleaning\"\n",
    "                )\n",
    "                data.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "\n",
    "    @ensure_annotations\n",
    "    def clean_text(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        def clean_text(text):\n",
    "            return clean(\n",
    "                text,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=False,\n",
    "                no_line_breaks=False,\n",
    "                no_urls=True,\n",
    "                no_emails=True,\n",
    "                no_phone_numbers=True,\n",
    "                no_numbers=False,\n",
    "                no_digits=False,\n",
    "                no_currency_symbols=False,\n",
    "                no_punct=False,\n",
    "                no_emoji=True,\n",
    "                replace_with_url=\"<URL>\",\n",
    "                replace_with_phone_number=\"<PHONE>\",\n",
    "                replace_with_email=\"<EMAIL>\",\n",
    "            )\n",
    "\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "\n",
    "        if \"sample_submission.csv\" not in filename:\n",
    "            data[\"body\"] = data[\"body\"].apply(clean_text)\n",
    "            for key, dtype in data.dtypes.items():\n",
    "                if is_string_dtype(dtype):\n",
    "                    data[key] = data[key].apply(clean_text)\n",
    "            logger.info(f\"Washed the file : {'.'.join(filename)}\")\n",
    "            self.deduplication(data, path, name=\"\", save=False)\n",
    "        else:\n",
    "            logger.warning(f\"Couldn't clean text in {'.'.join(filename)}\")\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "\n",
    "    @ensure_annotations\n",
    "    def zero_shot_transform(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        try:\n",
    "            if \"sample_submission.csv\" not in filename:\n",
    "                features = self.config.features[filename[0]]\n",
    "\n",
    "                try:\n",
    "                    zeroshot = [data[features + [\"rule_violation\"]]]\n",
    "                except KeyError:\n",
    "                    zeroshot = []\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "                for violation in [\"positive\", \"negative\"]:\n",
    "                    for i in range(1, 3):\n",
    "                        temp = data[features[:-1] + [f\"{violation}_example_{i}\"]]\n",
    "                        temp[\"rule_violation\"] = 1 if violation == \"positive\" else 0\n",
    "                        temp = temp.rename(columns={f\"{violation}_example_{i}\": \"body\"})\n",
    "                        zeroshot.append(temp)\n",
    "\n",
    "                zeroshot = pd.concat(zeroshot, axis=0)\n",
    "                logger.info(f\"Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\")\n",
    "            else:\n",
    "                zeroshot = data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Error Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(zeroshot, target_dir.path / filename[-1])\n",
    "        return zeroshot\n",
    "\n",
    "    # @ensure_annotations\n",
    "    # def split_dataset(self, data: pd.DataFrame, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jigsaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
