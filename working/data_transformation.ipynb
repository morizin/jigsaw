{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7af18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b39a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53915642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.common import FilePath\n",
    "\n",
    "\n",
    "class DataSplitParams(BaseModel):\n",
    "    type: str\n",
    "    nsplits: int = 5\n",
    "    random_state: int = 2025\n",
    "\n",
    "\n",
    "class DataTransformationConfig(BaseModel):\n",
    "    outdir: Directory\n",
    "    indir: Directory\n",
    "    datasets: list[str]\n",
    "    splitter: DataSplitParams\n",
    "    features: dict[str, list[str]] | None\n",
    "    wash: bool\n",
    "    triplet: bool\n",
    "    zero: bool\n",
    "    matching: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import ConfigBox\n",
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "from src.jigsaw.utils.common import load_yaml, load_json\n",
    "from src.jigsaw.constants import *\n",
    "from src.jigsaw.entity.common import FilePath, Directory\n",
    "import os\n",
    "from src.jigsaw import logger\n",
    "# from src.jigsaw.entity.config_entity import DataSplitParams, DataTransformationConfig\n",
    "\n",
    "\n",
    "class ConfigurationManager(ConfigurationManager):\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        split_config = self.params.splitter\n",
    "        data_transform = self.config.data_transformation\n",
    "        splitter = DataSplitParams(\n",
    "            type=split_config.type,\n",
    "            nsplits=split_config.nsplits,\n",
    "            random_state=split_config.random_state,\n",
    "        )\n",
    "        status_file = load_json(\n",
    "            self.artifact_root.path\n",
    "            / os.path.join(self.config.data_validation.outdir, \"status.json\")\n",
    "        )\n",
    "\n",
    "        features = dict()\n",
    "        names = []\n",
    "        for name, schema in self.schema.items():\n",
    "            names.append(name)\n",
    "            if name in status_file:\n",
    "                for value in status_file[name].values():\n",
    "                    print(value)\n",
    "                    if value[\"data_redundancy\"]:\n",
    "                        features[name] = schema.features\n",
    "\n",
    "        return DataTransformationConfig(\n",
    "            outdir=Directory(path=self.artifact_root.path / data_transform.outdir),\n",
    "            indir=Directory(\n",
    "                path=self.artifact_root.path / self.config.data_ingestion.outdir\n",
    "            ),\n",
    "            datasets=names,\n",
    "            splitter=splitter,\n",
    "            features=features,\n",
    "            wash=data_transform.wash if hasattr(data_transform, \"wash\") else False,\n",
    "            triplet=data_transform.triplet\n",
    "            if hasattr(data_transform, \"triplet\")\n",
    "            else False,\n",
    "            zero=data_transform.zero if hasattr(data_transform, \"zero\") else False,\n",
    "            matching=data_transform.matching\n",
    "            if hasattr(data_transform, \"matching\")\n",
    "            else False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.config_entity import DataTransformationConfig, DataSplitParams\n",
    "\n",
    "from src.jigsaw.components.data.cleaning import remove_duplicates, clean_text\n",
    "from src.jigsaw.components.data.zeroshot import zero_shot_transform\n",
    "from src.jigsaw.components.data.folding import split_dataset\n",
    "from pathlib import Path\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_csv, print_format\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "        self.pipeline = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                self.pipeline.append((final_dir, remove_duplicates))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                self.pipeline.append((final_dir, clean_text))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_\" + final_dir\n",
    "                self.pipeline.append((final_dir, zero_shot_transform))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                self.pipeline.append((final_dir, list))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.pairwise:\n",
    "            for name in self.names:\n",
    "                final_dir = \"pairwise_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.splitter:\n",
    "            for name in self.names:\n",
    "                final_dir = \"folded_\" + final_dir\n",
    "                self.pipeline.append((final_dir, split_dataset))\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        for name in self.names:\n",
    "            for path in (self.indir / name).iterdir():\n",
    "                data = read_csv(path)\n",
    "                path = str(path).split(\"/\")[-2:]\n",
    "                for dirname, process in self.pipeline:\n",
    "                    data = process(\n",
    "                        config=self.config,\n",
    "                        data=data,\n",
    "                        path=path,\n",
    "                        name=dirname + name,\n",
    "                        outdir=self.outdir,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/morizin/Documents/Code/jigsaw-competition\")\n",
    "\n",
    "data = pd.read_csv(\"artifacts/data/zero_washed_cleaned_raw/train.csv\")\n",
    "\n",
    "((_, neg), (_, pos)) = data.groupby(by=\"rule_violation\")\n",
    "pos = pos.reset_index(drop=True)\n",
    "neg = neg.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a38f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataConfig(BaseModel):\n",
    "    ntriplets: int\n",
    "    nsamples: int\n",
    "    random_state: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jigsaw.entity.config_entity import DataTransformationConfig\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.utils.common import save_csv\n",
    "from pandas.core.frame import DataFrame\n",
    "from ensure import ensure_annotations\n",
    "from tqdm.autonotebook import tqdm\n",
    "from src.jigsaw import logger\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "\n",
    "@ensure_annotations\n",
    "def triplet_dataset(\n",
    "    config: DataTransformationComponent,\n",
    "    data: DataFrame,\n",
    "    path: list,\n",
    "    name: str,\n",
    "    outdir: str | Path | None = None,\n",
    ") -> DataFrame:\n",
    "    dataname, filename = path\n",
    "    triplet_config = config.triplet\n",
    "\n",
    "    if filename == \"sample_submission.csv\":\n",
    "        return data\n",
    "\n",
    "    if set(data.columns) != set(config.features + [config.targets]):\n",
    "        return data\n",
    "\n",
    "    if data.loc[0, \"rule_violation\"] == 0:\n",
    "        ((_, neg), (_, pos)) = data.groupby(\"rule_violation\")\n",
    "    else:\n",
    "        ((_, pos), (_, neg)) = data.groupby(\"rule_violation\")\n",
    "\n",
    "    pos = pos.reset_index(drop=True)\n",
    "    neg = neg.reset_index(drop=True)\n",
    "\n",
    "    rules = neg.groupby(\"rule\").apply(lambda x: list(x.sample(len(x)).index)).to_dict()\n",
    "\n",
    "    sr = (\n",
    "        neg.groupby([\"rule\", \"subreddit\"])\n",
    "        .apply(lambda x: list(x.sample(len(x)).index))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    pos_repeat = pd.concat([pos] * triplet_config.ntriplets, axis=0)\n",
    "    negatives = []\n",
    "\n",
    "    logger.info(\n",
    "        f\"Generating {triplet_config.nsamples} samples of {triplet_config.ntriplet} triplets each in the file {dataname}.{filename}\"\n",
    "    )\n",
    "    for idx, positive in tqdm(pos_repeat.iterrows(), total=len(pos_repeat)):\n",
    "        subred = sr.get((positive.rule, positive.subreddit), None)\n",
    "        chosen_idx = []\n",
    "        remaining = triplet_config.nsample\n",
    "        if subred:\n",
    "            idx = min(len(subred), remaining)\n",
    "            chosen_idx.extend(subred[:idx])\n",
    "            sr[(positive.rule, positive.subreddit)] = subred[idx:]\n",
    "            remaining -= idx\n",
    "\n",
    "        if remaining:\n",
    "            rule = rules[positive.rule]\n",
    "            idx = min(remaining, len(rule))\n",
    "            chosen_idx.extend(rule[:idx])\n",
    "            rules[positive.rule] = rule[idx:]\n",
    "            remaining -= idx\n",
    "\n",
    "        while remaining > 0:\n",
    "            rules = (\n",
    "                neg.groupby(\"rule\")\n",
    "                .apply(lambda x: list(x.sample(len(x)).index))\n",
    "                .to_dict()\n",
    "            )\n",
    "            rule = rules[positive.rule]\n",
    "            idx = min(remaining, len(rule))\n",
    "            chosen_idx.extend(rule[:idx])\n",
    "            rules[positive.rule] = rule[idx:]\n",
    "            remaining -= idx\n",
    "\n",
    "        negatives.append(chosen_idx)\n",
    "\n",
    "    negatives = pd.DataFrame(\n",
    "        [neg.loc[idx, \"body\"].values for idx in negatives],\n",
    "        columns=[f\"negative_{i}\" for i in range(len(negatives[0]))],\n",
    "        index=range(len(negatives)),\n",
    "    )\n",
    "\n",
    "    assert negatives.shape == (\n",
    "        pos.shape[0] * triplet_config.ntriplets,\n",
    "        triplet_config.nsample,\n",
    "    ), logger.error(\n",
    "        f\"Error when generating triplets '{dataname}.{filename}', shape doesn't match {negatives.shape} != ({pos.shape[0] * triplet_config.ntriplets}, {triplet_config.nsamples}\"\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Generating {triplet_config.nsamples} samples of {triplet_config.ntriplet} triplets each in the file {dataname}.{filename}\"\n",
    "    )\n",
    "\n",
    "    pos_repeat = pd.merge(pos_repeat, negatives)\n",
    "\n",
    "    if outdir:\n",
    "        target_dir = Directory(\n",
    "            path=(Path(outdir) if isinstance(outdir, str) else outdir) / name\n",
    "        )\n",
    "        save_csv(data, target_dir.path / filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259be495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/morizin/Documents/Code/jigsaw-competition\")\n",
    "\n",
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "from src.jigsaw.components.data.transformation import DataTransformationComponent\n",
    "\n",
    "cfg = ConfigurationManager()\n",
    "DataTransformationComponent(cfg.get_data_transformation_config())()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7446ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.jigsaw.config.config import ConfigurationManager\n",
    "# from src.jigsaw.components.data_transform import DataTranformationComponent\n",
    "\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.config = ConfigurationManager().get_data_transformation_config()\n",
    "        self.comp = DataTransformationComponent(self.config)\n",
    "\n",
    "    def kickoff(\n",
    "        self,\n",
    "    ):\n",
    "        self.comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataTransformationPipeline().kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import validate_call\n",
    "import pandas as pd\n",
    "from src.jigsaw import logger\n",
    "from src.jigsaw.entity.common import Directory\n",
    "from src.jigsaw.entity.config_entity import DataTransformationConfig, DataSplitParams\n",
    "\n",
    "from src.jigsaw.components.dataset.cleaning import remove_duplicates\n",
    "from pathlib import Path\n",
    "from ensure import ensure_annotations\n",
    "from cleantext import clean\n",
    "from pandas.api.types import is_string_dtype\n",
    "from src.jigsaw.utils.common import read_csv, save_csv, print_format\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        self.outdir = self.config.outdir.path\n",
    "        self.indir = self.config.indir.path\n",
    "\n",
    "        self.names = []\n",
    "\n",
    "        final_dir = \"\"\n",
    "\n",
    "        length = 100\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Available\", length)\n",
    "        print(\"=\" * length)\n",
    "        for name in self.config.datasets:\n",
    "            if (self.outdir / name).is_dir():\n",
    "                print_format(name, length)\n",
    "                self.names.append(str(name))\n",
    "        print(\"=\" * length)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"=\" * length)\n",
    "        print_format(\"Datasets Generating\", length)\n",
    "        print(\"=\" * length)\n",
    "        if self.config.features:\n",
    "            for name in self.names:\n",
    "                final_dir = \"cleaned_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.wash:\n",
    "            for name in self.names:\n",
    "                final_dir = \"washed_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.triplet:\n",
    "            for name in self.names:\n",
    "                final_dir = \"triplet_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.zero:\n",
    "            for name in self.names:\n",
    "                final_dir = \"zero_shot_\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        if self.config.pairwise:\n",
    "            for name in self.names:\n",
    "                final_dir = \"pairwise\" + final_dir\n",
    "                print_format(self.indir / f\"{final_dir}{name}/\", length)\n",
    "            print(\"=\" * length)\n",
    "\n",
    "        self.final_dir = final_dir\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self):\n",
    "        for dataset in self.names:\n",
    "            for file in (self.indir / dataset).iterdir():\n",
    "                final_dir = dataset\n",
    "                data = read_csv(file)\n",
    "                file = str(file).split(\"/\")[-2:]\n",
    "                if self.config.features:\n",
    "                    final_dir = \"cleaned_\" + final_dir\n",
    "                    data = remove_duplicates(\n",
    "                        self.config, data, file, final_dir, self.outdir\n",
    "                    )\n",
    "\n",
    "                if self.config.wash:\n",
    "                    final_dir = \"washed_\" + final_dir\n",
    "                    data = self.clean_text(data, file, name=final_dir)\n",
    "\n",
    "                if self.config.zero:\n",
    "                    final_dir = \"zero_\" + final_dir\n",
    "                    data = self.zero_shot_transform(data, file, name=final_dir)\n",
    "\n",
    "    @ensure_annotations\n",
    "    def deduplication(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        print(path, filename)\n",
    "        if filename[-1] != \"sample_submission.csv\":\n",
    "            features = self.config.features[filename[0]]\n",
    "            try:\n",
    "                data.drop_duplicates(subset=features, ignore_index=True, inplace=True)\n",
    "                logger.info(f\"cleaning out duplicates: {'.'.join(filename)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"Failed cleaning out duplicates: {'.'.join(filename)}\\nManual Cleaning\"\n",
    "                )\n",
    "                data.drop_duplicates(ignore_index=True, inplace=True)\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "\n",
    "    @ensure_annotations\n",
    "    def clean_text(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        def clean_text(text):\n",
    "            return clean(\n",
    "                text,\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                lower=False,\n",
    "                no_line_breaks=False,\n",
    "                no_urls=True,\n",
    "                no_emails=True,\n",
    "                no_phone_numbers=True,\n",
    "                no_numbers=False,\n",
    "                no_digits=False,\n",
    "                no_currency_symbols=False,\n",
    "                no_punct=False,\n",
    "                no_emoji=True,\n",
    "                replace_with_url=\"<URL>\",\n",
    "                replace_with_phone_number=\"<PHONE>\",\n",
    "                replace_with_email=\"<EMAIL>\",\n",
    "            )\n",
    "\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "\n",
    "        if \"sample_submission.csv\" not in filename:\n",
    "            data[\"body\"] = data[\"body\"].apply(clean_text)\n",
    "            for key, dtype in data.dtypes.items():\n",
    "                if is_string_dtype(dtype):\n",
    "                    data[key] = data[key].apply(clean_text)\n",
    "            logger.info(f\"Washed the file : {'.'.join(filename)}\")\n",
    "            self.deduplication(data, path, name=\"\", save=False)\n",
    "        else:\n",
    "            logger.warning(f\"Couldn't clean text in {'.'.join(filename)}\")\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(data, target_dir.path / filename[1])\n",
    "        return data\n",
    "\n",
    "    @ensure_annotations\n",
    "    def zero_shot_transform(\n",
    "        self, data: pd.DataFrame, path: Path | str, name: str, save: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        filename = str(path).split(\"/\")[-2:]\n",
    "        try:\n",
    "            if \"sample_submission.csv\" not in filename:\n",
    "                features = self.config.features[filename[0]]\n",
    "\n",
    "                try:\n",
    "                    zeroshot = [data[features + [\"rule_violation\"]]]\n",
    "                except KeyError:\n",
    "                    zeroshot = []\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "                for violation in [\"positive\", \"negative\"]:\n",
    "                    for i in range(1, 3):\n",
    "                        temp = data[features[:-1] + [f\"{violation}_example_{i}\"]]\n",
    "                        temp[\"rule_violation\"] = 1 if violation == \"positive\" else 0\n",
    "                        temp = temp.rename(columns={f\"{violation}_example_{i}\": \"body\"})\n",
    "                        zeroshot.append(temp)\n",
    "\n",
    "                zeroshot = pd.concat(zeroshot, axis=0)\n",
    "                logger.info(f\"Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\")\n",
    "            else:\n",
    "                zeroshot = data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Error Tranforming to Zero-Shot Dataset : {'.'.join(filename)}\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        if save:\n",
    "            target_dir = Directory(path=self.outdir / name)\n",
    "            save_csv(zeroshot, target_dir.path / filename[-1])\n",
    "        return zeroshot\n",
    "\n",
    "    # @ensure_annotations\n",
    "    # def split_dataset(self, data: pd.DataFrame, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jigsaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
